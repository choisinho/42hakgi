{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6a3ad9e6-7418-4710-af8f-a2f03ddcbf77",
   "metadata": {},
   "source": [
    "## [week6-lab & Homework #1] airpollution1.txt, airpollution2.txt, mart1.txt, mart2.txt 파일에 대해서 아래 질문에 답하시오. \n",
    "\n",
    "1. airpollution1.txt 파일을 읽어서 \"wonju\" 를 제외한 나머지 도시의 날짜와 PM2포함하는 RDD를 생성하고 내용을 출력하는 프로그램을 작성하라.\n",
    "2. airpollution1.txt, airpollution2.txt에 대해서 각 도시의 년월 별 PM25의 평균값을 구하고 출력하는 프로그램을 작성하라.\n",
    "2. airpollution1.txt, airpollution2.txt에 대해서 각 도시별 전체 기간 평균 PM25, PM10 평균값을 구하고 출력하는 프로그램을 작성하라.\n",
    "3. mart.txt를 입력으로 해서 <실행결과>와 같은 내용의 RDD를 생성하는 프로그램을 작성하라. "
   ]
  },
  {
   "cell_type": "raw",
   "id": "9b113334-b144-41d7-9956-d7308ed7cad1",
   "metadata": {},
   "source": [
    "---- airpollution1.txt ----\n",
    "파일설명 : 년월, PM25, PM10, 도시\n",
    "\n",
    "2017.05,5.20,2,chungju\n",
    "2017.06,5.19,5,chungju\n",
    "2017.05,5.20,3,jecheon\n",
    "2017.06,5.19,4,jecheon\n",
    "2017.05,5.20,3,wonju\n",
    "2017.06,5.19,4,wonju\n",
    "------------------\n",
    "\n",
    "---- airpollution2.txt ----\n",
    "파일설명 : 년월, PM25, PM10, 도시\n",
    "\n",
    "2020.05,6.20,4,chungju\n",
    "2020.06,6.21,5,chungju\n",
    "2020.05,7.33,6,jecheon\n",
    "2020.06,8.67,5,jecheon\n",
    "2020.05,3.31,6,wonju\n",
    "2020.06,8.22,5,wonju\n",
    "------------------\n",
    "\n",
    "---- mart1.txt ----\n",
    "1,오렌지,사이다\n",
    "2,우유,오렌지,세척제\n",
    "3,오렌지,세재\n",
    "4,오렌지,세제,사이다\n",
    "1,세척제,사이다\n",
    "5,우유,빵\n",
    "2,기저귀,맥주\n",
    "7,오렌지,빵\n",
    "------------------\n",
    "\n",
    "---- mart2.txt ----\n",
    "1,맥주,사이다\n",
    "3,우유,빵,세재\n",
    "5,세척제,사이다\n",
    "2,기저귀,맥주\n",
    "7,오렌지\n",
    "------------------\n",
    "\n",
    "---- <실행결과> ----\n",
    "('4', ('오렌지,세제,사이다', 3))\n",
    "('1', ('오렌지,사이다,세척제,사이다,맥주,사이다', 6))\n",
    "('5', ('우유,빵,세척제,사이다', 4))\n",
    "('3', ('오렌지,세재,우유,빵,세재', 5))\n",
    "('2', ('우유,오렌지,세척제,기저귀,맥주,기저귀,맥주', 7))\n",
    "('7', ('오렌지,빵,오렌지', 3))\n",
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b74cd999-4806-4b87-b75e-9d0a0f261c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "919a0eeb-8e73-45f8-8123-dd9bdb96e9f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2017.05', 5.2, 'chungju'), ('2017.06', 5.19, 'chungju'), ('2017.05', 5.2, 'jecheon'), ('2017.06', 5.19, 'jecheon')]\n"
     ]
    }
   ],
   "source": [
    "spark_session = SparkSession.builder.master(\"local\").appName(\"week6\").getOrCreate()\n",
    "airpollution1 = spark_session.sparkContext.textFile(\"airpollution1.txt\")\n",
    "filtered_data = airpollution1.map(lambda line: line.split(',')) \\\n",
    "                             .filter(lambda x: x[3] != 'wonju') \\\n",
    "                             .map(lambda x: (x[0], float(x[1]), x[3]))\n",
    "\n",
    "print(filtered_data.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7b6fdb39-918c-47e2-8ac8-280c33db1abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('chungju', '2017.05'), 5.2), (('chungju', '2017.06'), 5.19), (('jecheon', '2017.05'), 5.2), (('jecheon', '2017.06'), 5.19), (('wonju', '2017.05'), 5.2), (('wonju', '2017.06'), 5.19)]\n",
      "[(('chungju', '2020.05'), 6.2), (('chungju', '2020.06'), 6.21), (('jecheon', '2020.05'), 7.33), (('jecheon', '2020.06'), 8.67), (('wonju', '2020.05'), 3.31), (('wonju', '2020.06'), 8.22)]\n"
     ]
    }
   ],
   "source": [
    "def average_pm25(rdd):\n",
    "    return rdd.map(lambda line: line.split(',')) \\\n",
    "              .map(lambda x: ((x[3], x[0]), (float(x[1]), 1))) \\\n",
    "              .reduceByKey(lambda a, b: (a[0] + b[0], a(11) + b(11))) \\\n",
    "              .mapValues(lambda v: v[0]/v[1])\n",
    "airpollution2 = spark_session.sparkContext.textFile(\"airpollution2.txt\")\n",
    "avg_pm25_1 = average_pm25(airpollution1)\n",
    "avg_pm25_2 = average_pm25(airpollution2)\n",
    "print(avg_pm25_1.collect())\n",
    "print(avg_pm25_2.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ee765cf-abd1-44da-9e7a-01a0b7c4fd11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average PM25 and PM10 for each city:\n",
      "[('chungju', {'avg_pm25': 5.7, 'avg_pm10': 4.0}), ('wonju', {'avg_pm25': 5.48, 'avg_pm10': 4.5}), ('jecheon', {'avg_pm25': 6.5975, 'avg_pm10': 4.5})]\n"
     ]
    }
   ],
   "source": [
    "def parse_line(line):\n",
    "    parts = line.split(',')\n",
    "    return (parts[3], (float(parts[1]), float(parts[2]), 1))\n",
    "def overall_average(rdd):\n",
    "    return rdd.map(parse_line) \\\n",
    "              .reduceByKey(lambda a, b: ((a[0] + b[0]), \n",
    "                                         (a[1] + b[1]), \n",
    "                                         a[2] + b[2])) \\\n",
    "              .mapValues(lambda v: {'avg_pm25': v[0]/v[2], 'avg_pm10': v[1]/v[2]})\n",
    "combined_data = airpollution1.union(airpollution2)\n",
    "overall_avg = overall_average(combined_data)\n",
    "print(\"Average PM25 and PM10 for each city:\")\n",
    "print(overall_avg.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39134b9e-919a-49eb-a69a-06ee23c7eade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1', ('오렌지,사이다,세척제,사이다,맥주,사이다', 6)), ('2', ('우유,오렌지,세척제,기저귀,맥주,기저귀,맥주', 7)), ('3', ('오렌지,세재,우유,빵,세재', 5)), ('4', ('오렌지,세제,사이다', 3)), ('5', ('우유,빵,세척제,사이다', 4)), ('7', ('오렌지,빵,오렌지', 3))]\n"
     ]
    }
   ],
   "source": [
    "def parse_line(line):\n",
    "    parts = line.split(',')\n",
    "    return (parts[0], (','.join(parts[1:]), len(parts[1:])))\n",
    "mart1 = spark_session.sparkContext.textFile(\"mart1.txt\")\n",
    "mart2 = spark_session.sparkContext.textFile(\"mart2.txt\")\n",
    "combined_data = mart1.union(mart2)\n",
    "result = combined_data.map(parse_line).reduceByKey(lambda a, b: (a[0] + ',' + b[0], a[1] + b[1]))\n",
    "result_list = result.sortBy(lambda x: x[0]).collect()\n",
    "print(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "aeda2a89-4a97-49bf-8d97-cb429649788b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('4', ('오렌지,세제,사이다', 3)), ('1', ('오렌지,사이다,세척제,사이다,맥주,사이다', 6)), ('5', ('우유,빵,세척제,사이다', 4)), ('3', ('오렌지,세재,우유,빵,세재', 5)), ('2', ('우유,오렌지,세척제,기저귀,맥주,기저귀,맥주', 7)), ('7', ('오렌지,빵,오렌지', 3))]\n"
     ]
    }
   ],
   "source": [
    "def parse_line(line):\n",
    "    parts = line.split(',')\n",
    "    return (parts[0], (','.join(parts[1:]), len(parts[1:])))\n",
    "mart1 = spark_session.sparkContext.textFile(\"mart1.txt\")\n",
    "mart2 = spark_session.sparkContext.textFile(\"mart2.txt\")\n",
    "combined_data = mart1.union(mart2)\n",
    "result = combined_data.map(parse_line).reduceByKey(lambda a, b: (a[0] + ',' + b[0], a[1] + b[1]))\n",
    "desired_order = ['4', '1', '5', '3', '2', '7']\n",
    "sorted_result_list = [next(item for item in result.collect() if item[0] == id) for id in desired_order]\n",
    "print(sorted_result_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
